{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd07f4d97f22a9a973173b012f9bc9d2a139f43a30261cf742acddd600eb9c24920",
   "display_name": "Python 3.8.8 64-bit ('learning': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "7f4d97f22a9a973173b012f9bc9d2a139f43a30261cf742acddd600eb9c24920"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Linear Model and Regularization\n",
    "\n",
    "Linear model is one of the most simple machine learning algorithm. People often getting attracted to more advanced model such as Neural Network or Gradient Boosting due to the hype and the preditictive performance. However, on most of daily business case, building a linear model is good enough. Linear model is also comes with the benefit of being interpretable, compared to the black box Neural Network. On this occasion, we will build a linear model with the addition of regularization to analyze the data while still get a great predictive performance.\n",
    "\n",
    "## Library and Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Regex\n",
    "import re\n",
    "\n",
    "# Statistics\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Data Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Machine Learning Model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "source": [
    "# Case Study: Laptop Price Prediction\n",
    "\n",
    "On this occasion, we will try to understand what makes a price of a laptop to increase by building a linear model. For a computer geek or people who manufactured laptops may already now the production cost of each component. However, for a lay people like us who only know how to use the laptop, exploring this dataset and building a machine learning model around it will help us to compare laptop with various specifications and build by various companies. We may also see some intangible factors that can affect the price, such as the value of a brand like Apple or the CPU component such us Intel Core vs AMD.\n",
    "\n",
    "## Data\n",
    "\n",
    "The data come from [Laptop Company Price List](https://www.kaggle.com/muhammetvarl/laptop-price) with the following dictionary:\n",
    "\n",
    "- **Company**: Laptop Manufacturer\n",
    "- **Product**: Brand and Model\n",
    "- **TypeName**: Type (Notebook, Ultrabook, Gaming, etc.)\n",
    "- **Inches**: Screen Size\n",
    "- **ScreenResolution**: Screen Resolution\n",
    "- **Cpu**: Central Processing Unit (CPU)\n",
    "- **Ram**: Laptop RAM\n",
    "- **Memory**: Hard Disk / SSD Memory\n",
    "- **GPU**: Graphics Processing Units (GPU)\n",
    "- **OpSys**: Operating System\n",
    "- **Weight**: Laptop Weight\n",
    "- **Price_euros**: Price in Euro"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop = pd.read_csv('data/laptop_price.csv')\n",
    "\n",
    "laptop.head()"
   ]
  },
  {
   "source": [
    "Let's check the dimension of the data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop.shape"
   ]
  },
  {
   "source": [
    "Let's check the data type of each column. See if there is any incompatible column data type."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop.dtypes"
   ]
  },
  {
   "source": [
    "Some column should be numerical, such as the RAM and memory. Those columns is still in string format and have non-numeric characters. We will clean the data later before building the model.\n",
    "\n",
    "Let's check if there is any duplicated data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop[ laptop.duplicated()].shape"
   ]
  },
  {
   "source": [
    "Based on the finding, we will have 0 observation of duplicated data.\n",
    "\n",
    "Let's check if there is any missing value from each column."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop.isnull().sum(axis = 0)"
   ]
  },
  {
   "source": [
    "Based on the result, we find that there is no missing value in any column on our dataset.\n",
    "\n",
    "## Data Wrangling\n",
    "\n",
    "Although the information given from the dataset is quite comprehensive, we need to transform the data to a proper format to build a machine learning.\n",
    "\n",
    "### Transforming Weight\n",
    "\n",
    "The first we do is removing the weight unit (kg) from the `Weight` column and tranform the value into float/numeric."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \"kg\" from the weight\n",
    "laptop['Weight'] = list(map(lambda x: float(re.sub('kg', '', x)), laptop['Weight']))\n",
    "\n",
    "laptop.head()"
   ]
  },
  {
   "source": [
    "Let's check if there is any missing value as a result of our data wrangling process on the `Weight` column."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop['Weight'].isnull().sum()"
   ]
  },
  {
   "source": [
    "### Transforming RAM\n",
    "\n",
    "The next thing we do is removing the unit `GB` from the `Ram` column and transform the value into integer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop['Ram'] = list(map(lambda x: int(re.sub('GB', '', x)), laptop['Ram']))\n",
    "\n",
    "laptop.head()"
   ]
  },
  {
   "source": [
    "### Transforming Memory\n",
    "\n",
    "Now we will separate the `Memory` column into 3 different columns: SSD, HDD, and Flash based on the type of the storage system. The first thing we do is to find the specific storage type, for example SSD, and extract the value. If a laptop does not have any SSD, the value will be empty.\n",
    "\n",
    "Here I use the regex pattern `\\d+` which means digits (0-9) followed by the unit size of the storage (GB/TB) and ended with SSD to indicate that we only looking for SSD system."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ssd = list(map(lambda x: re.findall('\\d+GB SSD|\\d+TB SSD', x), laptop['Memory']))\n",
    "\n",
    "temp_ssd[0:10]"
   ]
  },
  {
   "source": [
    "The next thing we do is converting the string into proper numeric storage size value. If we find laptop with a TB size of storage, we will convert it into GB by multiply the value with 1000. Some laptops may have 2 separate SSD embedded, such as 256GB SSD + 256GB SSD. To simplify the problem, we just sum the value."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ssd = []\n",
    "\n",
    "for i in range(len(temp_ssd)):\n",
    "\n",
    "    for j in range(len(temp_ssd[i])):\n",
    "        if re.search('TB', temp_ssd[i][j]):\n",
    "            temp_ssd[i][j] = int(re.sub('TB SSD', '', temp_ssd[i][j]))*1000 # Convert TB to GB\n",
    "        else:\n",
    "            temp_ssd[i][j] = int(re.sub('GB SSD', '', temp_ssd[i][j])) \n",
    "    \n",
    "    final_ssd.append( np.sum(temp_ssd[i])) # Sum the SSD Memory Storage\n",
    "\n",
    "final_ssd[0:10]"
   ]
  },
  {
   "source": [
    "We will do the same thing with the HDD and Flash Storage."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDD Storage\n",
    "temp_hdd = list(map(lambda x: re.findall('\\d+GB HDD|\\d+TB HDD', x), laptop['Memory']))\n",
    "final_hdd = []\n",
    "\n",
    "for i in range(len(temp_hdd)):\n",
    "\n",
    "    for j in range(len(temp_hdd[i])):\n",
    "        if re.search('TB', temp_hdd[i][j]):\n",
    "            temp_hdd[i][j] = int(re.sub('TB HDD', '', temp_hdd[i][j]))*1000 # Convert TB to GB\n",
    "        else:\n",
    "            temp_hdd[i][j] = int(re.sub('GB HDD', '', temp_hdd[i][j])) \n",
    "    \n",
    "    final_hdd.append( np.sum(temp_hdd[i])) # Sum the total hdd Memory Storage\n",
    "\n",
    "# Flash Storage\n",
    "temp_flash = list(map(lambda x: re.findall('\\d+GB Flash|\\d+TB Flash', x), laptop['Memory']))\n",
    "final_flash = []\n",
    "\n",
    "for i in range(len(temp_flash)):\n",
    "\n",
    "    for j in range(len(temp_flash[i])):\n",
    "        if re.search('TB', temp_flash[i][j]):\n",
    "            temp_flash[i][j] = int(re.sub('TB Flash', '', temp_flash[i][j]))*1000 # Convert TB to GB\n",
    "        else:\n",
    "            temp_flash[i][j] = int(re.sub('GB Flash', '', temp_flash[i][j])) \n",
    "    \n",
    "    final_flash.append( np.sum(temp_flash[i])) # Sum the total flash Memory Storage"
   ]
  },
  {
   "source": [
    "Finally, we will attach the processed list into the initial `laptop` dataframe."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop['ssd'] = final_ssd\n",
    "laptop['hdd'] = final_hdd\n",
    "laptop['flash'] = final_flash\n",
    "\n",
    "laptop.head()"
   ]
  },
  {
   "source": [
    "### Transforming CPU\n",
    "\n",
    "The next thing we do is transforming the `Cpu` column. We will separate the processor type and the processor clock speed.\n",
    "\n",
    "The processor clock speed is indicated by the number followed by the GigaHertz (GHz) unit. Let's check the number of CPU type and their respective frequency of data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop.value_counts('Cpu')"
   ]
  },
  {
   "source": [
    "To simplify the processor/CPU type and prevent us from getting to many categorical class, we will only consider the general type only. For example, `Intel Core i5` and `Intel Core i5 7200U` will be considered as the same type of CPU. Let's check the result of the CPU type name cleansing process. We expect a general CPU type and try not to be to specific to reduce number of new features."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU type\n",
    "laptop_cpu = list(map(lambda x: re.findall('.*? \\d+', x)[0].strip(), laptop['Cpu']))\n",
    "laptop_cpu = list(map(lambda x: re.sub(' \\d+.*', '', x), laptop_cpu)) # Remove string started with numbers after whitespace\n",
    "laptop_cpu = list(map(lambda x: re.sub('[-].*', '', x), laptop_cpu)) # Remove type extension such as x-Z090 into x\n",
    "laptop_cpu = list(map(lambda x: re.sub(' [A-Z]\\d+.*', '', x), laptop_cpu)) # Remove string started with capital letters followed by numbers after whitespace\n",
    "\n",
    "pd.DataFrame(laptop_cpu).value_counts()"
   ]
  },
  {
   "source": [
    "We will continue by extracting the CPU clock speed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU clock speed\n",
    "laptop_cpu_clock = list(map(lambda x: float(re.sub('GHz', '', re.findall('\\d+GHz|\\d+[.]\\d+.*GHz', x)[0])), laptop['Cpu']))\n",
    "\n",
    "laptop_cpu_clock[0:10]"
   ]
  },
  {
   "source": [
    "After we have collected the list for the processor type and the processor clock speed, we attach them to the initial dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop['cpu_type'] = laptop_cpu\n",
    "laptop['cpu_clock'] = laptop_cpu_clock\n",
    "\n",
    "laptop.head(10)"
   ]
  },
  {
   "source": [
    "### Transforming GPU\n",
    "\n",
    "GPU is also an important part, especially for people who want to look for better gaming experience. Since there are a lot of GPU variant, we will only extract the first 2 words from the GPU type. For example, `Intel Iris` or `Intel HD`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "gpu_type = list(map(lambda x: \" \".join(x.split()[0:2]), laptop['Gpu']))\n",
    "\n",
    "laptop['gpu_type'] = gpu_type\n",
    "\n",
    "laptop.head()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### Transforming Screen Resolution\n",
    "\n",
    "Next, we will extract information from the `ScreenResolution`. If the laptop has touchscreen feature, we will give value of `1`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "touch_screen = []\n",
    "\n",
    "for i in range(len(laptop['ScreenResolution'])):\n",
    "    if re.search('Touchscreen', laptop['ScreenResolution'][i]):\n",
    "        touch_screen.append(1)\n",
    "    else:\n",
    "        touch_screen.append(0)\n",
    "\n",
    "touch_screen[0:20]"
   ]
  },
  {
   "source": [
    "Now we will extract the screen width. A special is when the screen resolution is in 4K, where the dimension is not explicitly stated. To counter such problem, we will assume that for all laptop with 4K resolution has aspect ratio of 16:9 or 3840x2160, which is the most common 4K resolution according to [PC Monitors](https://pcmonitors.info/articles/the-4k-uhd-3840-x-2160-experience/)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_width_str = list(map(lambda x: re.sub('x', '', re.findall('\\d+.*?x', x)[0]), laptop['ScreenResolution']))\n",
    "screen_width = []\n",
    "\n",
    "for i in range(len(screen_width_str)):\n",
    "    if re.search('4K', screen_width_str[i]):\n",
    "       screen_width.append(3840)\n",
    "    else: \n",
    "        screen_width.append(int(screen_width_str[i])) \n",
    "\n",
    "screen_width[0:10]"
   ]
  },
  {
   "source": [
    "We will continue extracting the height resolution of the screen."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_height_str = list(map(lambda x: re.sub('x', '', re.findall('x.*\\d+', x)[0]), laptop['ScreenResolution']))\n",
    "screen_height = []\n",
    "\n",
    "for i in range(len(screen_height_str)):\n",
    "    if re.search('4K', screen_height_str[i]):\n",
    "       screen_height.append(2160)\n",
    "    else: \n",
    "        screen_height.append(int(screen_height_str[i])) \n",
    "\n",
    "screen_height[0:10]"
   ]
  },
  {
   "source": [
    "We will also extract the type of the monitor. If an observation doesn't have any type of monitor and only show the screen resolution, we will fill the monitor type with `others`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_type = list(map(lambda x: re.sub('\\d+.*x.*', '', x), laptop['ScreenResolution']))\n",
    "monitor_type = list(map(lambda x: re.sub('Touchscreen', '', x), monitor_type))\n",
    "monitor_type = list(map(lambda x: re.sub('[/]', '', x).strip(), monitor_type))\n",
    "\n",
    "for i in range(len(monitor_type)):\n",
    "    if monitor_type[i] == '':\n",
    "        monitor_type[i] = 'others'\n",
    "\n",
    "pd.DataFrame(monitor_type).value_counts()"
   ]
  },
  {
   "source": [
    "Finally, we attach the screen information to the initial dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop['touchscreen'] = touch_screen\n",
    "laptop['screen_width'] = screen_width\n",
    "laptop['screen_height'] = screen_height\n",
    "laptop['monitor_type'] = monitor_type\n",
    "\n",
    "laptop.head()"
   ]
  },
  {
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "After we have completed the data wrangling process, we will continue exploring the information from the dataset. Understanding the data is crucial before we start to build the machine learning model.\n",
    "\n",
    "To simplify the dataset, we will drop some columns that are not necessary for building the model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop_clean = laptop.copy()\n",
    "laptop_clean.drop(['laptop_ID', 'Product', 'TypeName', 'ScreenResolution', 'Cpu', 'Memory', 'Gpu'], axis = 1, inplace = True)\n",
    "\n",
    "laptop_clean.head()"
   ]
  },
  {
   "source": [
    "### Price Between Companies\n",
    "\n",
    "Let's explore the distribution of laptop price from different companies, regardless of the laptop specs."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "sns.boxplot(x = 'Price_euros', y = 'Company', data = laptop)\n",
    "plt.xlabel('Price in Euros')\n",
    "plt.ylabel('Company')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "One interesting finding from this boxplot is that `Razer` has a great price range. However, most of its laptop is still more pricy than other brands. Popular laptop such as `Asus` and `Acer` has less median than `Apple` or `MSI`.\n",
    "\n",
    "### Correlation Between Variables\n",
    "\n",
    "First, we will check the correlation between the numeric features and the target variables. Blue color indicate positive correlation while red color indicate negative correlation. If two variables are not correlated, they will be shown as white."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = laptop_clean.drop('touchscreen', axis = 1).corr()\n",
    "\n",
    "plt.pcolor(corr_mat, cmap = 'RdBu')\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(list(corr_mat.columns))), labels= list(corr_mat.columns), rotation = 90)\n",
    "plt.yticks(range(len(list(corr_mat.columns))), labels= list(corr_mat.columns))\n",
    "plt.xlabel('')"
   ]
  },
  {
   "source": [
    "Based on the correlation matrix, we can see that the price (`Price_euros`) has a relatively strong correlation with the RAM while other features has low correlation with the price.\n",
    "\n",
    "### Operating System (OS)\n",
    "\n",
    "Next, we will check the number of each variant of the operating system (OS) of the laptop.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop_agg = laptop_clean[['Price_euros', 'OpSys']].groupby('OpSys').count().\\\n",
    "    rename(columns = {'Price_euros':'Frequency'}).sort_values(by = 'Frequency', ascending = False)\n",
    "\n",
    "plt.bar(x = laptop_agg.index, height = laptop_agg['Frequency'])\n",
    "\n",
    "# Insert text\n",
    "for i in range(laptop_agg.shape[0]):\n",
    "    plt.text(laptop_agg.index[i], laptop_agg['Frequency'][i], laptop_agg['Frequency'][i])\n",
    "\n",
    "plt.xticks(rotation = 90)\n",
    "plt.xlabel('OS')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Operating System')"
   ]
  },
  {
   "source": [
    "### CPU\n",
    "\n",
    "Next, we will check the frequency of each type of processor based on the CPU general type."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop_agg = laptop_clean[['Price_euros', 'cpu_type']].groupby('cpu_type').count().\\\n",
    "    rename(columns = {'Price_euros':'Frequency'}).sort_values(by = 'Frequency', ascending = False)\n",
    "\n",
    "plt.bar(x = laptop_agg.index, height = laptop_agg['Frequency'])\n",
    "\n",
    "# Insert text\n",
    "for i in range(laptop_agg.shape[0]):\n",
    "    plt.text(laptop_agg.index[i], laptop_agg['Frequency'][i], laptop_agg['Frequency'][i])\n",
    "\n",
    "plt.xticks(rotation = 90)\n",
    "plt.xlabel('CPU')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('CPU Type by Frequency')"
   ]
  },
  {
   "source": [
    "Intel Core series are the most frequent processor type in the market. There are some CPU type with only 1 or 2 observations, such as the Samsung Cortex. We will label CPU type as `other` for CPU with only  observation from the data.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_cpu = list(laptop_agg[ laptop_agg['Frequency'] == 1].index)\n",
    "\n",
    "id_pos = list(laptop_clean['cpu_type'][laptop_clean['cpu_type'].isin(low_cpu)].index)\n",
    "\n",
    "laptop_clean.loc[id_pos, 'cpu_type'] = '1_others'\n",
    "\n",
    "laptop_clean[['cpu_type']].value_counts()"
   ]
  },
  {
   "source": [
    "Let's check the price distribution for each CPU vendor."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data = laptop_clean, y = 'cpu_type', x = 'Price_euros')\n",
    "plt.ylabel('CPU Type')\n",
    "plt.xlabel('Price in Euro')"
   ]
  },
  {
   "source": [
    "Based on the boxplot, we can see that Intel Xeon, Intel Core i7, and AMD Ryzen has higher median compared to other processor. The most expensive laptops are build with Intel Core i7 based on the outliers.\n",
    "\n",
    "### GPU\n",
    "\n",
    "We will continue by checking the type of the GPU."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop_agg = laptop_clean[['Price_euros', 'gpu_type']].groupby('gpu_type').count().\\\n",
    "    rename(columns = {'Price_euros':'Frequency'}).sort_values(by = 'Frequency', ascending = False)\n",
    "\n",
    "plt.bar(x = laptop_agg.index, height = laptop_agg['Frequency'])\n",
    "\n",
    "# Insert text\n",
    "for i in range(laptop_agg.shape[0]):\n",
    "    plt.text(laptop_agg.index[i], laptop_agg['Frequency'][i], laptop_agg['Frequency'][i])\n",
    "\n",
    "plt.xticks(rotation = 90)\n",
    "plt.xlabel('GPU')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('GPU Type by Frequency')"
   ]
  },
  {
   "source": [
    "Intel HD is the most common, followed by the NVidia GeForce series. We will also group all GPU that only has 1 observation as `others`.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_gpu = list(laptop_agg[ laptop_agg['Frequency'] == 1].index)\n",
    "\n",
    "id_pos = list(laptop_clean[ laptop_clean['gpu_type'].isin(low_gpu)].index)\n",
    "\n",
    "laptop_clean.loc[id_pos, 'gpu_type'] = '1_others'\n",
    "\n",
    "laptop_clean[['gpu_type']].value_counts()"
   ]
  },
  {
   "source": [
    "We will also check the price distribution of each GPU type."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data = laptop_clean, y = 'gpu_type', x = 'Price_euros')\n",
    "plt.ylabel('GPU Vendor')\n",
    "plt.xlabel('Price in Euro')"
   ]
  },
  {
   "source": [
    "Based on the distribution of each boxplot, laptop with NVidia GPU is slightly more pricy compared to other vendor. From the outliers, combined with the information from the previous CPU vendor price distribution, we can see that laptop with Intel process and NVidia GPU has higher price. This is kinda expected since most gaming laptop tend to have NVidia GPU and Intel processor. To check this argument, we will explore the laptop with price higher than 3000 Euro and see the combination of the CPU and GPU."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop[['Company', 'Product', 'Cpu', 'Gpu']][ laptop['Price_euros'] > 3000]"
   ]
  },
  {
   "source": [
    "We can see that all of the laptop with price higher than 3000 Euro has Intel Core i7 or higher as the processor and NVidia as the GPU. We have done the exploratory data analysis to understand our data, now we will start to build the machine learning model.\n",
    "\n",
    "## One-Hot Encoding\n",
    "\n",
    "Before we split the data into data training and data testing, now we will convert the categorical variable into dummy features by using one-hot encoding so that it can be processed by the machine learing model.\n",
    "\n",
    "The following columns will be transformed:\n",
    "\n",
    "- cpu_type\n",
    "- gpu_type\n",
    "- OpSys (OS)\n",
    "- Company\n",
    "\n",
    "First, we will convert the category into integer number using the label encoding. For example, AMD will be 0, Intel will be 1, etc. After the data is converted, we then apply the one-hot encoding and convert the result into an array. The `drop='first'` will remove the first category from the encoding, so if we have 5 different categories in the column, we will only get 4 new columns from the result of one-hot encoding. Since we name the `others` category as `1_others`, it will be removed during the encoding process, allowing us to predict any new type of category that is not present in the current dataset.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Category into Integer\n",
    "cpu_label = LabelEncoder().fit(laptop_clean['cpu_type']).transform(laptop_clean['cpu_type'])\n",
    "cpu_label = cpu_label.reshape(len(laptop_clean['cpu_type']), 1)\n",
    "\n",
    "# Convert Label into One Hot Encoding\n",
    "cpu_label = OneHotEncoder(drop = 'first').fit(cpu_label).transform(cpu_label).toarray()\n",
    "\n",
    "cpu_label"
   ]
  },
  {
   "source": [
    "To help us during model interpretation, we will collect the cpu category as the column name for later purpose. One hot encoding will use alphabetical order everytime it convert the categorical data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_name = list(set(laptop_clean['cpu_type']))\n",
    "cpu_name.sort()\n",
    "cpu_name = list(map(lambda x: 'cpu_' + x, cpu_name))\n",
    "cpu_name = cpu_name[ 1:len(cpu_name) ]\n",
    "\n",
    "cpu_onehot = pd.DataFrame(cpu_label, columns = cpu_name)\n",
    "\n",
    "cpu_onehot.head()"
   ]
  },
  {
   "source": [
    "Finally, we will create a dataframe from the one hot encoding and add them into the dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop_clean = pd.concat([laptop_clean.reset_index(drop = True), cpu_onehot], axis = 1)\n",
    "\n",
    "laptop_clean.head()"
   ]
  },
  {
   "source": [
    "We will do the same thing for the GPU type, company and the OS."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Category into Integer\n",
    "gpu_label = LabelEncoder().fit(laptop_clean['gpu_type']).transform(laptop_clean['gpu_type'])\n",
    "gpu_label = gpu_label.reshape(len(laptop_clean['gpu_type']), 1)\n",
    "\n",
    "# Convert Label into One Hot Encoding\n",
    "gpu_label = OneHotEncoder(drop = 'first').fit(gpu_label).transform(gpu_label).toarray()\n",
    "\n",
    "# Create Column name\n",
    "gpu_name = list(set(laptop_clean['gpu_type']))\n",
    "gpu_name.sort()\n",
    "gpu_name = list(map(lambda x: 'gpu_' + x, gpu_name))\n",
    "gpu_name = gpu_name[ 1:len(gpu_name) ]\n",
    "\n",
    "# Concat the Column\n",
    "gpu_onehot = pd.DataFrame(gpu_label, columns = gpu_name)\n",
    "\n",
    "laptop_clean = pd.concat([laptop_clean.reset_index(drop = True), gpu_onehot], axis = 1)\n",
    "\n",
    "laptop_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Category into Integer\n",
    "os_label = LabelEncoder().fit(laptop_clean['OpSys']).transform(laptop_clean['OpSys'])\n",
    "os_label = os_label.reshape(len(laptop_clean['OpSys']), 1)\n",
    "\n",
    "# Convert Label into One Hot Encoding\n",
    "os_label = OneHotEncoder(drop = 'first').fit(os_label).transform(os_label).toarray()\n",
    "\n",
    "# Create Column name\n",
    "os_name = list(set(laptop_clean['OpSys']))\n",
    "os_name.sort()\n",
    "os_name = list(map(lambda x: 'os_' + x, os_name))\n",
    "os_name = os_name[ 1:len(os_name) ]\n",
    "\n",
    "# Concat the Column\n",
    "os_onehot = pd.DataFrame(os_label, columns = os_name)\n",
    "\n",
    "laptop_clean = pd.concat([laptop_clean.reset_index(drop = True), os_onehot], axis = 1)\n",
    "\n",
    "laptop_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Category into Integer\n",
    "company_label = LabelEncoder().fit(laptop_clean['Company']).transform(laptop_clean['Company'])\n",
    "company_label = company_label.reshape(len(laptop_clean['Company']), 1)\n",
    "\n",
    "# Convert Label into One Hot Encoding\n",
    "company_label = OneHotEncoder(drop = 'first').fit(company_label).transform(company_label).toarray()\n",
    "\n",
    "# Create Column name\n",
    "company_name = list(set(laptop_clean['Company']))\n",
    "company_name.sort()\n",
    "company_name = list(map(lambda x: 'company_' + x, company_name))\n",
    "company_name = company_name[ 1:len(company_name) ]\n",
    "\n",
    "# Concat the Column\n",
    "company_onehot = pd.DataFrame(company_label, columns = company_name)\n",
    "\n",
    "laptop_clean = pd.concat([laptop_clean.reset_index(drop = True), company_onehot], axis = 1)\n",
    "\n",
    "laptop_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Category into Integer\n",
    "monitor_label = LabelEncoder().fit(laptop_clean['monitor_type']).transform(laptop_clean['monitor_type'])\n",
    "monitor_label = monitor_label.reshape(len(laptop_clean['monitor_type']), 1)\n",
    "\n",
    "# Convert Label into One Hot Encoding\n",
    "monitor_label = OneHotEncoder(drop = 'first').fit(monitor_label).transform(monitor_label).toarray()\n",
    "\n",
    "# Create Column name\n",
    "monitor_name = list(set(laptop_clean['monitor_type']))\n",
    "monitor_name.sort()\n",
    "monitor_name = list(map(lambda x: 'monitor_' + x, monitor_name))\n",
    "monitor_name = monitor_name[ 1:len(monitor_name) ]\n",
    "\n",
    "# Concat the Column\n",
    "monitor_onehot = pd.DataFrame(monitor_label, columns = monitor_name)\n",
    "\n",
    "laptop_clean = pd.concat([laptop_clean.reset_index(drop = True), monitor_onehot], axis = 1)\n",
    "\n",
    "laptop_clean.head()"
   ]
  },
  {
   "source": [
    "Finally, we will once again drop the unncessary columns and only take the numeric columns."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop_clean = laptop_clean.select_dtypes(include = 'number')\n",
    "laptop_clean.columns = list(map(lambda x: re.sub(' ', '_', x), laptop_clean.columns))\n",
    "\n",
    "laptop_clean.head()"
   ]
  },
  {
   "source": [
    "## Train Test Split\n",
    "\n",
    "We will start splitting the data intro training and testing dataset. We will use 20% of the data as the testing dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_laptop = laptop_clean.drop('Price_euros', axis = 1)\n",
    "y_laptop = laptop_clean['Price_euros']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_laptop, y_laptop, test_size = 0.2, random_state = 100)\n",
    "\n",
    "print(\"Number of Data Train: \" + str(x_train.shape[0]))\n",
    "print(\"Number of Data Test: \" + str(x_test.shape[0]))"
   ]
  },
  {
   "source": [
    "## Model Fitting\n",
    "\n",
    "We will start building machine learning model. We will build the following model and compare the predictive performance:\n",
    "\n",
    "- Linear Regression\n",
    "- Lasso Regression \n",
    "- Ridge Regression \n",
    "- Elastic Net Regression\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "First, we fit the OLS (Ordinary Least Square) linear regression into the training dataset. OLS will try to find the best coefficient for the intercept and each feature by minimizing the **Sum of Squared Error** as the lost function.\n",
    "\n",
    "$$\n",
    "SSE = \\Sigma_{i=1}^n (y - \\overline y)^2\n",
    "$$\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "lm_model = LinearRegression().fit(x_train, y_train)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "# Save Model\n",
    "import pickle\n",
    "\n",
    "pkl_filename = \"output/linear_reg.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(lm_model, file)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "Let's check the estimate coefficients for each features, see how big the association between the features and the laptop price. We will only highlight features with the highest (top 10) and the lowest (bottom 10) coefficient value due to limited visualization space."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_lm = pd.DataFrame({'features': x_train.columns, 'estimate':lm_model.coef_}).sort_values(by = 'estimate', ascending = False)\n",
    "top_last10 = coef_lm.iloc[np.r_[0:10, -10:0]]\n",
    "\n",
    "sns.barplot(data = top_last10, x = 'estimate', y = 'features')\n",
    "plt.xlabel('Estimate Coefficients')\n",
    "plt.ylabel('Features')"
   ]
  },
  {
   "source": [
    "Based on the top 10 highest and lowest coefficient of each feature, we can see that certain type of CPUs will lower the predicted price due to the negative value of the estimate coefficient. For example, laptop installed with AMD FX or Intel Pentium Dual Core will have lower price compared to laptop that is installed with Intel Xeon. If the laptop is build by Razer, the predicted price will increase by around 1250 Euros.\n",
    "\n",
    "#### Model Evaluation\n",
    "\n",
    "Let's check the prediction performance of the linear regression. We will use the R-squared (R2 Score) and the error measured by Root Mean Squared Error (RMSE). RMSE is a good measure to evaluate regression problem because they punish model more if there are observations that has high error."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lm = lm_model.predict(x_test)\n",
    "\n",
    "print('R2 Score: ' + str(np.round(r2_score(y_test, pred_lm), 3)))\n",
    "print('RMSE: ' + str(np.round(np.sqrt(mean_squared_error(y_test, pred_lm)), 3)) )\n",
    "print('Price Standard Deviation: ' + str(np.round(np.std(y_test), 3)))"
   ]
  },
  {
   "source": [
    "We can compare the RMSE with the standard deviation of the price variable from the testing dataset. According to [Bowles](https://www.amazon.com/Machine-Learning-Spark-Python-Techniques/dp/1119561930), if the RMSE is lower than the standard deviation, then we can conclude that the model has a good performance. A good model should, on average, have better predictions than the naive estimate of the mean for all predictions.\n",
    "\n",
    "### Lasso Regression\n",
    "\n",
    "Lasso regression is a variant of linear regression that comes with a penalty on the loss function to help the model do regularization and reduce the model variance. Model with less variance will be better at predicing new data. The idea is to induce the penalty against complexity by adding the regularization term such as that with increasing value of regularization parameter, the weights get reduced (and, hence penalty induced).\n",
    "\n",
    "As you may have learn before, linear regression try to get the best estimate value for the model intercept and slope for each feature by minimizing the Sum of Squared Error (SSE). \n",
    "\n",
    "$$\n",
    "SSE = \\Sigma_i^N (y_i - \\hat y_i)^2\n",
    "$$\n",
    "\n",
    "Lasso Regression will add an L1 penalty with $\\lambda$ constant to the loss function. If $\\lambda$ equals zero, then the lasso regression become identical with the ordinary linear regression.\n",
    "\n",
    "$$\n",
    "SSE = \\Sigma_{i=1}^N (y_i - \\hat y_i)^2 + \\lambda\\ \\Sigma_{j=1}^n |\\beta_j|\n",
    "$$\n",
    "\n",
    "The benefit of using Lasso is that it can function as a feature selection method. This model will shrink and sometimes remove features so that we only have the features that affect the target data. To fit a Lasso model, we need to scale all features. The features need to have the same scale so that the coefficient values are chosen based only on which attribute is most useful, not on the basis of which one has the most favorable scale."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale Features\n",
    "x_scaler = StandardScaler().fit(x_train.to_numpy())\n",
    "\n",
    "# Transform Data\n",
    "x_train_norm = x_scaler.transform(x_train.to_numpy())\n",
    "x_test_norm = x_scaler.transform(x_test.to_numpy())"
   ]
  },
  {
   "source": [
    "The first thing we need to do to build a Lasso model is by choosing the appropriate value of $\\lambda$ as the penalty constant. Luckily, the `sklearn` package has build in estimator can help us get the optimal hyper-parameter (in this case, $\\lambda$ or $\\alpha$) with Cross-Validation method to evaluate the model.\n",
    "\n",
    "In the following step, we set 10-Fold Cross-Validation method to fit and evaluate the data and try 1000 different alpha ($\\lambda$) as the penalty constant. The model will give us the best alpha to choose."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model_cv = LassoCV(cv = 10, n_alphas = 1000).fit(x_train_norm, y_train)\n",
    "\n",
    "print('Best alpha: ' + str(np.round(lasso_model_cv.alpha_, 5)))"
   ]
  },
  {
   "source": [
    "We can directly predict new data using the previously fitted model. Let's evaluate the model on the unseen testing dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lasso = lasso_model_cv.predict(x_test_norm)\n",
    "\n",
    "print('R2 Score: ' + str(np.round(r2_score(y_test, pred_lasso), 3)))\n",
    "print('RMSE: ' + str(np.round(np.sqrt(mean_squared_error(y_test, pred_lasso)), 3)) )\n",
    "print('Price Standard Deviation: ' + str(np.round(np.std(y_test), 3)))"
   ]
  },
  {
   "source": [
    "You can also try to refit the data into new Lasso model with the best alpha as the input. The result is the same."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model = Lasso(alpha = lasso_model_cv.alpha_).fit(x_train_norm, y_train)\n",
    "\n",
    "pred_lasso = lasso_model.predict(x_test_norm)\n",
    "\n",
    "print('R2 Score: ' + str(np.round(r2_score(y_test, pred_lasso), 3)))\n",
    "print('RMSE: ' + str(np.round(np.sqrt(mean_squared_error(y_test, pred_lasso)), 3)) )\n",
    "print('Price Standard Deviation: ' + str(np.round(np.std(y_test), 3)))"
   ]
  },
  {
   "source": [
    "Let's visualize on how the different value of $\\lambda$ will affect the estimate of coefficient for each feature. Here we use $\\lambda$ from 0.0001 to 600 and fit the lasso regression."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nbins = 1000\n",
    "list_lambda = np.linspace(1e-4, 600, nbins)\n",
    "\n",
    "list_coef = np.zeros((nbins, x_train.shape[1]))\n",
    "\n",
    "for i in range(len(list_lambda)):\n",
    "    lasso_reg = Lasso(alpha = list_lambda[i]).fit(x_train_norm, y_train)\n",
    "    list_coef[i, :] = lasso_reg.coef_\n",
    "\n",
    "df_coef = pd.DataFrame(list_coef, columns = x_train.columns)\n",
    "\n",
    "df_coef.head()"
   ]
  },
  {
   "source": [
    "Now we will visualize the result."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_coef.columns:\n",
    "    plt.plot(list_lambda, df_coef[i])\n",
    "plt.xlabel('Lambda Hyper-Parameter')\n",
    "plt.ylabel('Standardized Coefficients')"
   ]
  },
  {
   "source": [
    "With bigger $\\lambda$, more features will be omitted or will have estimate coefficient of 0 and only retain the most important features. With $\\lambda$ = 400, only 1 feature remain.\n",
    "\n",
    "Now check the remaining feature for $\\lambda$ > 100. Note that the coefficient is already normalized."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('lambda: ' + str(list_lambda[200]))\n",
    "print('\\nRemaining features')\n",
    "df_coef.iloc[200][ np.abs(df_coef.iloc[200]) > 0]"
   ]
  },
  {
   "source": [
    "Let's check the remaining feature for $\\lambda$ > 200."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('lambda: ' + str(list_lambda[340]))\n",
    "print('\\nRemaining features')\n",
    "df_coef.iloc[300][ np.abs(df_coef.iloc[300]) > 0]"
   ]
  },
  {
   "source": [
    "### Ridge Regression\n",
    "\n",
    "Ridge regression is similar with Lasso by creating a penalty toward the lost function. The difference is that the ridge regression will square the coefficient instead of making it absolute for the penalty. Larger value of $\\lambda$ will make the coefficient to be smaller, but never reach to 0 in Ridge regression.\n",
    "\n",
    "$$\n",
    "SSE = \\Sigma_{i=1}^N (y_i - \\hat y_i)^2 + \\lambda\\ \\Sigma_{j=1}^n \\beta_j^2\n",
    "$$\n",
    "\n",
    "In the following process, I set the possible alpha values from 0.0001 to 100 with different steps."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_range = [1e-4, 1e-3, 1e-2, 0.1, 1]\n",
    "alpha_range.extend(np.arange(10, 100, 1))\n",
    "\n",
    "ridge_model_cv = RidgeCV(cv = 10, alphas = alpha_range).fit(x_train_norm, y_train)\n",
    "\n",
    "ridge_model_cv.alpha_"
   ]
  },
  {
   "source": [
    "Let's evaluate the model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ridge = ridge_model_cv.predict(x_test_norm)\n",
    "\n",
    "print('R2 Score: ' + str(np.round(r2_score(y_test, pred_ridge), 3)))\n",
    "print('RMSE: ' + str(np.round(np.sqrt(mean_squared_error(y_test, pred_ridge)), 3)) )\n",
    "print('Price Standard Deviation: ' + str(np.round(np.std(y_test), 3)))"
   ]
  },
  {
   "source": [
    "If the lasso regression can remove unnecessary feature by making the coefficient to 0 one by one, ridge regression will shrink all coeffients but it will never reach absolute zero."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 1000\n",
    "list_lambda = np.linspace(1e-4, 1e5, nbins)\n",
    "\n",
    "list_coef = np.zeros((nbins, x_train.shape[1]))\n",
    "\n",
    "for i in range(len(list_lambda)):\n",
    "    ridge_reg = Ridge(alpha = list_lambda[i]).fit(x_train_norm, y_train)\n",
    "    list_coef[i, :] = ridge_reg.coef_\n",
    "\n",
    "df_coef = pd.DataFrame(list_coef, columns = x_train.columns)\n",
    "\n",
    "for i in df_coef.columns:\n",
    "    plt.plot(list_lambda, df_coef[i])\n",
    "plt.xlabel('Alpha Hyper-Parameter')\n",
    "plt.ylabel('Standardized Coefficients')"
   ]
  },
  {
   "source": [
    "### Elastic Net Regression\n",
    "\n",
    "Elastic Net combine both L1 penalty and the L2 penalty into a single formula. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge.\n",
    "\n",
    "In the following example, we can set the ratio between the L1 and L2 penalty. If `l1_ratio` = 0, then the model will be Ridge regression while if `l1_ratio` = 1 then the model become Lasso regression."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_model_cv = ElasticNetCV(l1_ratio = 0.5, n_alphas = 1000).fit(x_train_norm, y_train)\n",
    "\n",
    "elastic_model_cv.alpha_"
   ]
  },
  {
   "source": [
    "Let's evaluate the model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_elastic = elastic_model_cv.predict(x_test_norm)\n",
    "\n",
    "print('R2 Score: ' + str(np.round(r2_score(y_test, pred_elastic), 3)))\n",
    "print('RMSE: ' + str(np.round(np.sqrt(mean_squared_error(y_test, pred_elastic)), 3)) )\n",
    "print('Price Standard Deviation: ' + str(np.round(np.std(y_test), 3)))"
   ]
  },
  {
   "source": [
    "Different `l1_ratio` will give us different model performance."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_model_cv = ElasticNetCV(l1_ratio = 0.8, n_alphas = 1000).fit(x_train_norm, y_train)\n",
    "\n",
    "pred_elastic = elastic_model_cv.predict(x_test_norm)\n",
    "\n",
    "print('R2 Score: ' + str(np.round(r2_score(y_test, pred_elastic), 3)))\n",
    "print('RMSE: ' + str(np.round(np.sqrt(mean_squared_error(y_test, pred_elastic)), 3)) )\n",
    "print('Price Standard Deviation: ' + str(np.round(np.std(y_test), 3)))"
   ]
  },
  {
   "source": [
    "We can put multiple `l1_ratio` to try with list."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_model_cv = ElasticNetCV(l1_ratio = [0.05, 0.1, 0.2, 0.3, 0.7, 0.8, 0.9, 0.95], n_alphas = 1000).fit(x_train_norm, y_train)\n",
    "\n",
    "pred_elastic = elastic_model_cv.predict(x_test_norm)\n",
    "\n",
    "print('Chosen L1 ratio: ', elastic_model_cv.l1_ratio_)\n",
    "print('R2 Score: ' + str(np.round(r2_score(y_test, pred_elastic), 3)))\n",
    "print('RMSE: ' + str(np.round(np.sqrt(mean_squared_error(y_test, pred_elastic)), 3)) )\n",
    "print('Price Standard Deviation: ' + str(np.round(np.std(y_test), 3)))"
   ]
  },
  {
   "source": [
    "# Conclusion\n",
    "\n",
    "Based on our result, all regularization method works better than the vanilla linear regression, with the Elastic Net achieve the lowest error on testing dataset. We also see that even with linear model we can achieve good result, as the RMSE of each model is still better than the standard deviation of the testing dataset. We have also learn how Lasso and Ridge regression remove or shrink the coefficient of each feature."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}